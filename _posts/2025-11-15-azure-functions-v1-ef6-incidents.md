When Legacy .NET Meets Azure Functions: Two Production Incidents and What They Taught Us

We recently lifted an old .NET Framework + Entity Framework 6 codebase into an Azure Functions v1 app that listens to Service Bus messages and writes to SQL Server.

On paper, the flow is straightforward:
	â€¢	New booking â†’ Service Bus topic
	â€¢	Azure Function picks up the message
	â€¢	We map it, update user/profile/payment tables via EF, and emit telemetry to Application Insights

In reality, we hit two very different issues that kept showing up in production:
	1.	Host lock lease failures on the Function host
	2.	Entity Framework â€œA second operation started on this contextâ€¦â€ errors under load

To make it more confusing, these two errors often appeared a day or two after a restart, so for a while we were convinced the host lock problem was causing the EF exceptions.

Below is what actually happened.

â¸»

Architecture at a Glance

flowchart LR
    A[External System] -->|Booking Message| B[Azure Service Bus Topic]
    B --> C[Azure Function App (v1, .NET Framework)]
    C -->|Host locks, logs| D[Storage Account (AzureWebJobsStorage)]
    C -->|EF6 / ADO.NET| E[SQL Database]
    C -->|Telemetry| F[Application Insights]


â¸»

Issue 1 â€“ â€œFailed to acquire host lock lease (403 Forbidden)â€

Symptoms

Every few seconds, the host spammed:

Host 'xxxx' failed to acquire host lock lease:
Microsoft.WindowsAzure.Storage: The remote server returned an error: (403) Forbidden.
Forbidden.
Forbidden.

Restarting the Function App â€œfixedâ€ it temporarily.

A day later, it crept back.

Our storage account security was tightened at the same time:
	â€¢	Public network access disabled
	â€¢	Only private endpoints and VNets were allowed

Everything seemed correct: keys, connection strings, RBAC, managed identity. But the host couldnâ€™t renew the lock blob in azure-webjobs-hosts/locks.

â¸»

Why the host needs a blob lease

Azure Functions uses a host lock (blob lease) so that:
	â€¢	Only one instance is â€œactiveâ€ when needed
	â€¢	Timers/triggers do not overlap incorrectly
	â€¢	Internal housekeeping routines coordinate safely

If the runtime canâ€™t access this blob â†’ host lock failure â†’ runtime goes unhealthy.

â¸»

Why we initially assumed this caused the EF errors
	â€¢	The issue always started after 24â€“48 hours.
	â€¢	Restarting the Function App fixed it every time.
	â€¢	We were restarting the function during off-hours just to keep the system stable.

This made us believe the host issue was triggering the other failures.

But the truth was more nuanced.

â¸»

Real Root Cause: Storage Network Rules Blocking Internal Azure IPs

After detailed discussions with Microsoft, the explanation was:

Azure Functions uses internal Azure IP ranges (not part of your VNet) to access the storage account.
If you set Public network access = Disabled, those internal IPs get blocked, even if the Function App is in the same VNet.

We asked:

â€œAre these Microsoft public IPs or private?â€

The answer:

They are internal Azure IPs, neither in your VNet nor truly â€œpublicâ€.
When you choose â€˜Disable public network accessâ€™, the storage firewall blocks them.
When you use Selected networks + your VNet, those internal calls continue to work.

The official docs do not explicitly state this, but the behavior is consistent with:
	â€¢	https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security-virtual-networks?tabs=azure-portal
	â€¢	https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security-set-default-access?tabs=azure-portal

â¸»

Fix: Switch From â€œDisabledâ€ to â€œSelected Networksâ€

To restore connectivity:
	1.	Open Storage Account â†’ Networking
	2.	Set:
Public Network Access = Enabled from selected networks and IP addresses
	3.	Add the same VNet/Subnet as the Function App
	4.	Keep private endpoints intact

This allowed both:
	â€¢	Our Function App traffic
	â€¢	Azure internal system traffic

Once done:
	â€¢	Host lock lease errors vanished
	â€¢	Function App stayed healthy permanently
	â€¢	We stopped scheduling midnight restarts to â€œstabilizeâ€ it

â¸»

How it works (visual)

sequenceDiagram
    participant Func as Azure Function Host
    participant SA as Storage Account
    participant Infra as Azure Internal Infra

    Note over Func,SA: With Public Access Disabled
    Func->>SA: Acquire host lock lease (blob)
    SA-->>Func: 403 Forbidden (internal IP blocked)

    Note over Func,SA: After Selected Networks + VNet
    Func->>SA: Acquire host lock lease
    SA-->>Func: 200 OK (lease acquired)


â¸»

Issue 2 â€“ EF6 + Azure Functions = â€œA second operation started on this contextâ€¦â€

Even after fixing host issues, another completely different error remained:

A second operation started on this context before a previous async operation completed.
Any instance members are not guaranteed to be thread-safe.

And sometimes deeper:

INSERT statement conflicted with the FOREIGN KEY constraint
FK_ICubePayment_UserProfile

This was happening inside our business logic, not at the platform level.

And again, restarting the function app made it go awayâ€¦ temporarily.

Which further fooled us into thinking the two issues were related.

â¸»

Root Cause: A Static Shared EF6 DbContext ğŸ¤¦â€â™‚ï¸

Our old Framework code used a static dependency container, which created one DbContext instance at cold start:

public static readonly BookingServicebusFunc BookingConsumer;

static DependencyContainer()
{
    var dbContext = new TVSModel(connectionString);
    BookingConsumer = new BookingServicebusFunc(dbContext, ...);
}

And our Azure Function used it like:

var consumer = DependencyContainer.BookingConsumer;
await consumer.ProcessOfflineBookingData(payload);

So:
	â€¢	One global shared DbContext
	â€¢	All Service Bus messages, across all threads, using that same context
	â€¢	EF6 is NOT thread-safe, even with async/await everywhere

Under load:
	â€¢	Message #1 saving a UserProfile
	â€¢	Message #2 saving a Payment
	â€¢	Both using the same DbContext
	â€¢	EF internal state gets corrupted

Leading to:
	â€¢	â€œsecond operation started on this contextâ€
	â€¢	Foreign key conflicts
	â€¢	Partial writes
	â€¢	Random failures after some uptime

Restarting the app reset the static context â†’ making the problem â€œgo awayâ€ until next time.

â¸»

We Tried Everything Before Finding the Real Fix

Because the error text talks about async operations, we naturally went in that direction:
	â€¢	Converted everything to async / await
	â€¢	Switched to SaveChangesAsync() everywhere
	â€¢	Added checks and tried to make sure every EF call was awaited properly
	â€¢	Added more detailed status logging like status:Mapped Payload to see where it blew up

But even with everything awaited, the error kept coming back under load.

Thatâ€™s when we had to accept the more fundamental truth:

Entity Framework 6 DbContext is not thread-safe.
Even if every method is async/await, you cannot safely share one context instance across concurrent operations.

In an Azure Functions app, multiple messages are processed in parallel. By sharing a static context, we had:
	â€¢	Message 1 querying and saving on the same TVSModel
	â€¢	Message 2 doing the same at the same time
	â€¢	EFâ€™s change tracker and connection state getting corrupted

Result: â€œsecond operation started on this contextâ€ and FK conflicts.

â¸»

Final Fix: One DbContext Per Message + Proper Disposal

1. DependencyContainer now CREATES a new DbContext per call

public static BookingServicebusFunc CreateBookingConsumer()
{
    var dbContext = new TVSModel(_connectionString);
    return new BookingServicebusFunc(
        dbContext,
        new BookingDetailRepository(dbContext),
        new UserProfileRepository(dbContext),
        ...
    );
}


â¸»

2. BookingServicebusFunc implements IDisposable

public class BookingServicebusFunc : IDisposable
{
    private readonly TVSModel _tvsDbContext;
    private bool _disposed;

    public BookingServicebusFunc(TVSModel tvsDbContext, /* ... */)
    {
        _tvsDbContext = tvsDbContext;
        // ...
    }

    public void Dispose()
    {
        Dispose(true);
        GC.SuppressFinalize(this);
    }

    protected virtual void Dispose(bool disposing)
    {
        if (!_disposed)
        {
            if (disposing)
            {
                _tvsDbContext?.Dispose();
            }
            _disposed = true;
        }
    }

    // all your async methods using _tvsDbContext...
}


â¸»

3. Azure Function uses using(...) per invocation

[FunctionName("ProcessBookingFunction")]
public static async Task Run(
    [ServiceBusTrigger(
        "%BookingServiceBusTopic%",
        "%BookingServiceBusSubscriptionName%",
        AccessRights.Listen,
        Connection = "BookingServiceBusConnectionString"
    )] string mySbMsg,
    TraceWriter log)
{
    log.Info($"ğŸ“¨ Received booking message: {mySbMsg}");

    using (var consumer = DependencyContainer.CreateBookingConsumer())
    {
        string status = "NotStarted";
        var payloads = consumer.DeserializeBookingPayload(mySbMsg);

        foreach (var payload in payloads)
        {
            try
            {
                var result = await consumer.ProcessOfflineBookingData(payload);
                status = result?.ToString() ?? "null";
                log.Info($"Processed booking {payload.BookingUUID} with status: {status}");
            }
            catch (Exception ex)
            {
                log.Error($"âŒ Error processing booking {payload.BookingUUID} with status : {status}: {ex.Message}");
            }
        }
    }   // DbContext disposed here
}

This ensures:
	â€¢	No shared DbContext
	â€¢	Clean unit-of-work per trigger execution
	â€¢	No concurrency or tracking conflicts

After this change:
	â€¢	The â€œsecond operation started on this contextâ€ error stopped completely
	â€¢	The FK conflicts vanished
	â€¢	We could remove the off-hours restart hack entirely

â¸»

Putting Both Issues Side by Side

flowchart TB
    subgraph Issue1[Issue #1: Host Lock / Storage Networking]
      I1A[Public network access disabled] --> I1B[Internal Azure IPs blocked]
      I1B --> I1C[403 on host lock lease]
      I1C --> I1D[Host becomes unhealthy after a day]
      I1D --> I1E[Manual restart temporarily hides issue]
    end

    subgraph Issue2[Issue #2: EF Concurrency]
      I2A[Static shared DbContext] --> I2B[Parallel message processing]
      I2B --> I2C[Second operation on this context]
      I2C --> I2D[FK conflicts & partial writes]
      I2D --> I2E[Restart resets DbContext and hides issue]
    end

Both issues produced the same symptom:

â¡ â€œEverything works fine after a restart, then breaks after some hours/days.â€

But the root causes were completely different.

â¸»

Lessons Learned

1. Azure Storage networking can impact the Function Runtime itself

Not just your code.

Prefer:

âœ” Selected networks + VNet
Instead of
âŒ Hard â€œDisable Public Accessâ€

Docs:
https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security-virtual-networks
https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security-set-default-access

â¸»

2. EF6 DbContext is a unit-of-work â€” not a singleton

In Azure Functions:
	â€¢	Create 1 DbContext per invocation
	â€¢	Dispose it after use
	â€¢	Never share it across messages

â¸»

3. Beware of â€œrestart-driven debuggingâ€

If an app works after restart and fails after N hours/days:
	â€¢	Something has a wrong lifetime
	â€¢	Something is cached incorrectly
	â€¢	Or the platform configuration is blocking long-running behaviour

A restart is a symptom, not a solution.

â¸»

If you want, I can also prepare:

âœ” A shortened LinkedIn version
âœ” A two-part series split
âœ” A GitHub README variant
âœ” A version with callouts & warnings for internal Confluence

Just tell me!
